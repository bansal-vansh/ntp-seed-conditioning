{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8ba6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Config\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "device = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['HF_HOME'] = \"/scratch/cluster/vansh/hf_cache\"\n",
    "\n",
    "from data.dataset import CustomTokenizer\n",
    "from data.circle import make_dataset, compute_canonical_permutation\n",
    "from model.networks import AttentionOnlyLMHeadModel\n",
    "from model.eval import decode_batch, generate_samples, visualize_attention_weights\n",
    "from model.train import train_main\n",
    "from model.utils import set_seed, get_model, get_tokenizer, _plot_attention_grid\n",
    "from data.dataset import collate_fn\n",
    "\n",
    "# %%\n",
    "DEVICE = torch.device(f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d37e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset parameters\n",
    "dataset_name = \"circle\"\n",
    "model_type = \"gpt2\"\n",
    "model_path = None\n",
    "\n",
    "M = 15\n",
    "N = 9\n",
    "H = 26\n",
    "HL = 5\n",
    "seed_per_pi = False\n",
    "num_train_samples =10000\n",
    "batch_size = 64\n",
    "num_eval_samples = 1000\n",
    "eval_batch_size = 256\n",
    "num_ckpts = 20\n",
    "epochs = 200\n",
    "eval_runs = 1\n",
    "top_p=1.0\n",
    "save_name = f\"debug/M{M}-N{N}-H{H}-NT{num_train_samples}-E{epochs}-top_p{top_p}-{model_type}\"\n",
    "data_root = f\"/datastor1/vansh/lang_sampling/data\"\n",
    "regenerate_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dff6684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(model_type=model_type)\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1b7241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS token: <BOS>, ID: 19\n",
      "EOS token: <EOS>, ID: 20\n",
      "PAD token: <EOS>, ID: 20\n",
      "Loading existing dataset from /datastor1/vansh/lang_sampling/data/circle/M15-N9\n",
      "Pre-calculating all training seeds...\n",
      "Pre-calculating all training seeds...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(args, DEVICE, custom=True)\n",
    "train_dataset, test_dataset, tokenizer, train_strs, train_perms, VOCAB, SEED_TOKENS, EVAL_TOKENIZER = make_dataset(\n",
    "                                                                                                                    M=M,\n",
    "                                                                                                                    N=N,\n",
    "                                                                                                                    H=H,\n",
    "                                                                                                                    seed_len=HL,\n",
    "                                                                                                                    seed_per_pi=seed_per_pi,\n",
    "                                                                                                                    num_train_samples=num_train_samples,\n",
    "                                                                                                                    num_test_samples=num_eval_samples,\n",
    "                                                                                                                    tokenizer=tokenizer,\n",
    "                                                                                                                    data_root=data_root,\n",
    "                                                                                                                    regenerate=regenerate_data,\n",
    "                                                                                                                    add_new_tokens=True\n",
    "                                                                                                                )\n",
    "VOCAB_IDs = {EVAL_TOKENIZER._convert_token_to_id(tok) for tok in VOCAB}\n",
    "data_collator = lambda features: collate_fn(features, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4c531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new gpt2 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(47, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#             \"google/gemma-2b\",\n",
    "#             cache_dir=\"/scratch/cluster/vansh/hf_cache\",\n",
    "#             torch_dtype=torch.bfloat16,\n",
    "#             attn_implementation=\"eager\",\n",
    "#             ).to(DEVICE)\n",
    "model = get_model(args, tokenizer, DEVICE, n_embed=768, n_layer=4, n_head=3)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8dc04af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Learning Rate Finder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      " 94%|█████████▍| 94/100 [00:16<00:01,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 3.59E-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder plot saved to lr_finder.png\n",
      "\n",
      "--- LR Finder complete. You can now use the suggested LR for training. ---\n"
     ]
    }
   ],
   "source": [
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "class HFTrainLoaderIter(TrainDataLoaderIter):\n",
    "    def inputs_labels_from_batch(self, batch_data):\n",
    "        return batch_data, batch_data[\"labels\"]\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "def hf_criterion(outputs, labels):\n",
    "    return outputs.loss\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size, # Use the same batch size as your main training\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# --- LR Finder ---\n",
    "print(\"\\n--- Running Learning Rate Finder ---\")\n",
    "\n",
    "# 1. Wrap the model\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "# 2. Define optimizer\n",
    "optimizer = optim.Adam(wrapped_model.parameters(), lr=1e-7, weight_decay=0.01)\n",
    "\n",
    "# 3. Wrap the DataLoader\n",
    "hf_train_iter = HFTrainLoaderIter(train_loader)\n",
    "\n",
    "# 4. Initialize and run the finder\n",
    "lr_finder = LRFinder(wrapped_model, optimizer, hf_criterion, device=DEVICE)\n",
    "lr_finder.range_test(hf_train_iter, end_lr=1, num_iter=100)\n",
    "\n",
    "# 5. Plot and reset\n",
    "lr_finder.plot() # Saves lr_finder.png\n",
    "print(\"LR Finder plot saved to lr_finder.png\")\n",
    "lr_finder.reset()\n",
    "\n",
    "print(\"\\n--- LR Finder complete. You can now use the suggested LR for training. ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b74f202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ccdf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v0 v10 v12 v0 v5 v4 v6 v1 v8 v12 v10 v14 v1 v5 v4 v8 v14 v6',\n",
       " 'v13 v6 v14 v0 v12 v11 v11 v14 v6 v7 v8 v1 v1 v13 v0 v8 v7 v12',\n",
       " 'v3 v6 v4 v3 v10 v0 v6 v10 v13 v9 v9 v4 v5 v8 v0 v5 v8 v13',\n",
       " 'v2 v5 v6 v1 v0 v13 v3 v12 v11 v3 v13 v6 v12 v2 v1 v11 v5 v0',\n",
       " 'v12 v10 v8 v13 v4 v8 v10 v11 v14 v9 v11 v4 v13 v2 v2 v14 v9 v12']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.strings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff0a156e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[19, 44, 42, 46, 45, 25,  4, 14, 16,  4,  9,  8, 10,  5, 12, 16, 14, 18,\n",
       "           5,  9,  8, 12, 18, 10, 20],\n",
       "         [19, 23, 24, 25, 31, 36,  6,  9, 10,  5,  4, 17,  7, 16, 15,  7, 17, 10,\n",
       "          16,  6,  5, 15,  9,  4, 20]]),\n",
       " 'labels': tensor([[-100, -100, -100, -100, -100, -100,    4,   14,   16,    4,    9,    8,\n",
       "            10,    5,   12,   16,   14,   18,    5,    9,    8,   12,   18,   10,\n",
       "            20],\n",
       "         [-100, -100, -100, -100, -100, -100,    6,    9,   10,    5,    4,   17,\n",
       "             7,   16,   15,    7,   17,   10,   16,    6,    5,   15,    9,    4,\n",
       "            20]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn([train_dataset[0], train_dataset[3]], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8174aab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS> <H23> <H21> <H25> <H24> <H4> v0 v10 v12 v0 v5 v4 v6 v1 v8 v12 v10 v14 v1 v5 v4 v8 v14 v6 <EOS>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([19, 44, 42, 46, 45, 25,  4, 14, 16,  4,  9,  8, 10,  5, 12, 16, 14, 18,\n",
    "           5,  9,  8, 12, 18, 10, 20])\n",
    "# tokenizer.decode([662, 1729,    4, 2510])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82581dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training permutations set size: 8870\n"
     ]
    }
   ],
   "source": [
    "train_perms_set = set(train_perms)\n",
    "print(f\"Training permutations set size: {len(train_perms_set)}\")\n",
    "\n",
    "def is_coherent_after_walk(seq, N, vocab_ids=VOCAB_IDs) -> bool:\n",
    "    \"\"\"\n",
    "    Given that π exists (walk succeeded), check coherence:\n",
    "    - correct number of edges\n",
    "    - correct tokens\n",
    "    \"\"\"\n",
    "    if len(seq) != 2 * N:\n",
    "        return False\n",
    "\n",
    "    vocab_set = set(vocab_ids)\n",
    "    if any(token not in vocab_set for token in seq):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def evaluate_model(model, \n",
    "                greedy, \n",
    "                temperature, \n",
    "                label, \n",
    "                decode_fn, \n",
    "                train_dataset, \n",
    "                seed=42, \n",
    "                batch_size=eval_batch_size, \n",
    "                num_eval_samples=num_eval_samples,\n",
    "                top_p=top_p):\n",
    "    set_seed(seed)\n",
    "    print(f\"\\nEvaluating {label} for seed {seed}...\")\n",
    "\n",
    "    samples, outputs, input_lengths  = generate_samples(model, \n",
    "                                        train_dataset, \n",
    "                                        tokenizer,\n",
    "                                        decode_fn=decode_fn, \n",
    "                                        greedy=greedy, \n",
    "                                        seed_tokens=SEED_TOKENS, \n",
    "                                        seed_len=HL, \n",
    "                                        max_length=train_dataset[1][\"labels\"].shape[0]*2,\n",
    "                                        temperature=temperature, \n",
    "                                        top_p=top_p, \n",
    "                                        num_samples=num_eval_samples, \n",
    "                                        batch_size=batch_size\n",
    "                                        )\n",
    "    tokenized_samples = [EVAL_TOKENIZER.encode(s) for s in samples]\n",
    "\n",
    "    unique_perms = set()\n",
    "    unique_coherent_perms = set()\n",
    "    num_coherent = 0\n",
    "    incoherent_samples = []\n",
    "    # Final metrics loop\n",
    "    for s in tokenized_samples:\n",
    "        pi = compute_canonical_permutation(s, N=N)\n",
    "        if pi is not None:\n",
    "            unique_perms.add(pi)\n",
    "            if is_coherent_after_walk(s, N=N, vocab_ids=VOCAB_IDs):\n",
    "                num_coherent += 1\n",
    "                unique_coherent_perms.add(pi)\n",
    "            else:\n",
    "                incoherent_samples.append(s)\n",
    "        else:\n",
    "            incoherent_samples.append(s)\n",
    "\n",
    "    num_memorized = len([s for s in unique_coherent_perms if s in train_perms_set])\n",
    "    num_creative = len([s for s in unique_coherent_perms if s not in train_perms_set])\n",
    "    num_unique = len(unique_perms)\n",
    "\n",
    "    representation_power = (num_memorized / len(samples))\n",
    "    creativity = (num_creative / len(samples)) \n",
    "    uniqueness = (num_unique / len(samples)) \n",
    "    coherence = (num_coherent / len(samples)) \n",
    "    # perplexity = compute_perplexity(model, test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # print(f\"Perplexity: {perplexity:.4f}\")\n",
    "    print(f\"Coherence: {coherence:.4f} ({num_coherent}/{len(samples)})\")\n",
    "    print(f\"Representation power: {representation_power:.4f} ({num_memorized}/{len(samples)})\")\n",
    "    print(f\"Creativity: {creativity:.4f} ({num_creative}/{len(samples)})\")\n",
    "    print(f\"Uniqueness: {uniqueness:.4f} ({num_unique}/{len(samples)})\")\n",
    "\n",
    "    print(\"Generated samples:\", samples[:5])\n",
    "    return np.array([\n",
    "        representation_power,\n",
    "        creativity,\n",
    "        uniqueness,\n",
    "        coherence,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1037d484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8501, 'grad_norm': 2.1948249340057373, 'learning_rate': 4.996974522292994e-05, 'epoch': 0.12738853503184713}\n",
      "{'loss': 2.595, 'grad_norm': 1.4879429340362549, 'learning_rate': 4.993789808917197e-05, 'epoch': 0.25477707006369427}\n",
      "{'loss': 2.5819, 'grad_norm': 1.2038607597351074, 'learning_rate': 4.9906050955414014e-05, 'epoch': 0.3821656050955414}\n",
      "{'loss': 2.5787, 'grad_norm': 1.5140697956085205, 'learning_rate': 4.987420382165605e-05, 'epoch': 0.5095541401273885}\n",
      "{'loss': 2.5711, 'grad_norm': 1.456027865409851, 'learning_rate': 4.984235668789809e-05, 'epoch': 0.6369426751592356}\n",
      "{'loss': 2.5404, 'grad_norm': 1.996604323387146, 'learning_rate': 4.9810509554140126e-05, 'epoch': 0.7643312101910829}\n",
      "{'loss': 2.437, 'grad_norm': 2.852355718612671, 'learning_rate': 4.977866242038217e-05, 'epoch': 0.89171974522293}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_main\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_eval_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_runs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_ckpts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_to_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lang_sampling/model/train.py:181\u001b[0m, in \u001b[0;36mtrain_main\u001b[0;34m(model, dataset_name, save_name, batch_size, num_epochs, temperatures, num_eval_runs, train_dataset, data_collator, device, decode_fn, eval_fn, num_checkpoints, save_results, num_workers, log_to_wandb, lr, eval_batch_size, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m every_n_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_steps \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_checkpoints)\n\u001b[1;32m    168\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m LiveEvalCallback(\n\u001b[1;32m    169\u001b[0m     eval_fn\u001b[38;5;241m=\u001b[39meval_fn,\n\u001b[1;32m    170\u001b[0m     log_fn\u001b[38;5;241m=\u001b[39mlog_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m     eval_batch_size\u001b[38;5;241m=\u001b[39meval_batch_size\n\u001b[1;32m    179\u001b[0m )\n\u001b[0;32m--> 181\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last_model_path:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the last model to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/lang_sampling/model/train.py:121\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, data_collator, device, batch_size, num_epochs, eval_callback, num_workers, lr, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m     93\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     94\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     95\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# max_grad_norm=1.0, \u001b[39;00m\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    115\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    116\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[eval_callback],\n\u001b[1;32m    120\u001b[0m )\n\u001b[0;32m--> 121\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/scratch/cluster/vansh/miniconda/envs/ml/lib/python3.10/site-packages/transformers/trainer.py:2206\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/cluster/vansh/miniconda/envs/ml/lib/python3.10/site-packages/transformers/trainer.py:2553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_main(\n",
    "    model=model,\n",
    "    dataset_name=dataset_name,\n",
    "    save_name=save_name,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=epochs,\n",
    "    temperatures=[0.3, 0.5, 0.7, 1.0, 2.0],\n",
    "    num_eval_runs=eval_runs,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    device=DEVICE,\n",
    "    decode_fn=None,\n",
    "    eval_fn=None,\n",
    "    num_checkpoints=num_ckpts,\n",
    "    log_to_wandb=False,\n",
    "    save_results=False, \n",
    "    lr=5e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a3d3701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating something for seed 42...\n",
      "\n",
      "Generating 1000 unconditional samples.\n",
      "Coherence: 0.2370 (237/1000)\n",
      "Representation power: 0.0560 (56/1000)\n",
      "Creativity: 0.1740 (174/1000)\n",
      "Uniqueness: 0.2300 (230/1000)\n",
      "Generated samples: ['v7 v14 v5 v13 v8 v6 v1 v7 v14 v8 v6 v1 v13 v12 v12 v10 v10 v5', 'v1 v14 v9 v8 v5 v1 v8 v10 v14 v11 v13 v6 v10 v13 v6 v5 v11 v9', 'v3 v2 v11 v12 v2 v11 v12 v8 v13 v4 v8 v13 v10 v6 v4 v10 v6 v3', 'v2 v0 v1 v9 v9 v12 v6 v2 v0 v8 v12 v6 v8 v5 v5 v4 v4 v1', 'v13 v12 v9 v13 v8 v6 v4 v2 v2 v0 v12 v8 v0 v9 v6 v10 v10 v4']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.056, 0.174, 0.23 , 0.237])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, greedy=False, temperature=0.5, label=\"something\", decode_fn=decode_batch, train_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d600a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14,\n",
       " 15,\n",
       " 15,\n",
       " 18,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 5,\n",
       " 5,\n",
       " 16,\n",
       " 14,\n",
       " 15,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 11,\n",
       " 3,\n",
       " 3,\n",
       " 11,\n",
       " 3,\n",
       " 17,\n",
       " 5,\n",
       " 18,\n",
       " 17,\n",
       " 15,\n",
       " 8]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVAL_TOKENIZER.encode(\"v10 v11 v11 v14 v12 v13 v14 v1 v1 v12 v10 v11 v2 v2 v15 v7 v16 v15 v7 v16 v13 v1 v14 v13 v11 v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c17abc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_canonical_permutation([14,\n",
    " 15,\n",
    " 15,\n",
    " 18,\n",
    " 16,\n",
    " 17,\n",
    " 18,\n",
    " 5,\n",
    " 5,\n",
    " 16,\n",
    " 14,\n",
    " 15,\n",
    " 6,\n",
    " 6,\n",
    " 3,\n",
    " 11,\n",
    " 3,\n",
    " 3,\n",
    " 11,\n",
    " 3,\n",
    " 17,\n",
    " 5,\n",
    " 18,\n",
    " 17,\n",
    " 15,\n",
    " 8], N=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e171e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 5 unconditional samples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['2 v5',\n",
       "  '5 v4 v9',\n",
       "  '0 v2reale Schnee bibliote2',\n",
       "  ' v14 v2 v13 v0 v2 v14 v1 v4 v0 v12 v4 v12 v3 v3 v12 v13 v11 v9 v9 v3 v11 v2 v7 v7 v3',\n",
       "  ' v12 v12 v7 v14 v6 v6 v8 v2 v4 v8 v2 v4 v14 v3 v3 v13 v13 v11 v14 v13 v3 v2 v8 v12 v12 v9 v9 v3 v4 v6 v0 v0 v3'],\n",
       " {'input_ids': tensor([[     2, 256000, 235284,    593, 235308, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001],\n",
       "          [     2, 256000, 235308,    593, 235310,    593, 235315, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001],\n",
       "          [     2, 256000, 235276,    593, 235284, 214060, 122237,  80621, 235284,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001],\n",
       "          [     2, 256000,    593, 235274, 235310,    593, 235284,    593, 235274,\n",
       "           235304,    593, 235276,    593, 235284,    593, 235274, 235310,    593,\n",
       "           235274,    593, 235310,    593, 235276,    593, 235274, 235284,    593,\n",
       "           235310,    593, 235274, 235284,    593, 235304,    593, 235304,    593,\n",
       "           235274, 235284,    593, 235274, 235304,    593, 235274, 235274,    593,\n",
       "           235315,    593, 235315,    593, 235304,    593, 235274, 235274,    593,\n",
       "           235284,    593, 235324,    593, 235324,    593, 235304, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001,\n",
       "           256001, 256001, 256001, 256001, 256001, 256001, 256001, 256001],\n",
       "          [     2, 256000,    593, 235274, 235284,    593, 235274, 235284,    593,\n",
       "           235324,    593, 235274, 235310,    593, 235318,    593, 235318,    593,\n",
       "           235321,    593, 235284,    593, 235310,    593, 235321,    593, 235284,\n",
       "              593, 235310,    593, 235274, 235310,    593, 235304,    593, 235304,\n",
       "              593, 235274, 235304,    593, 235274, 235304,    593, 235274, 235274,\n",
       "              593, 235274, 235310,    593, 235274, 235304,    593, 235304,    593,\n",
       "           235284,    593, 235321,    593, 235274, 235284,    593, 235274, 235284,\n",
       "              593, 235315,    593, 235315,    593, 235304,    593, 235310,    593,\n",
       "           235318,    593, 235276,    593, 235276,    593, 235304, 256001]],\n",
       "         device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')},\n",
       " [2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_samples(model, train_dataset, tokenizer, max_length=100, num_samples=5, top_p=1.0, temperature=1.0, decode_fn=\n",
    "                 decode_batch, greedy=False, batch_size=256, seed_tokens=SEED_TOKENS, seed_len=HL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a55e8b",
   "metadata": {},
   "source": [
    "samples, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b284d5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating 100 samples for attention analysis at step 100 ---\n",
      "Processing 16 samples (Batch 1)\n",
      "\n",
      "Generating 16 unconditional samples.\n",
      "Processing 16 samples (Batch 2)\n",
      "\n",
      "Generating 16 unconditional samples.\n",
      "Processing 16 samples (Batch 3)\n",
      "\n",
      "Generating 16 unconditional samples.\n",
      "Processing 16 samples (Batch 4)\n",
      "\n",
      "Generating 16 unconditional samples.\n",
      "Processing 16 samples (Batch 5)\n",
      "\n",
      "Generating 16 unconditional samples.\n",
      "Processing 16 samples (Batch 6)\n",
      "\n",
      "Generating 16 unconditional samples.\n",
      "Processing 4 samples (Batch 7)\n",
      "\n",
      "Generating 4 unconditional samples.\n",
      "\n",
      "Saving plots to directory: None/step_100\n",
      "\n",
      "--- Processing Attention for Layer 0 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "--- Example Samples Being Averaged ---\n",
      "[1] '3 tuned v1 v1'\n",
      "[2] '1 v4 v11'\n",
      "[3] '1 v6 v11'\n",
      "[4] ' v5 v0 seriousness geben'\n",
      "[5] ' ctl2 v4 v7'\n",
      "------------------------------------\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer0_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 1 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer1_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 2 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer2_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 3 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer3_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 4 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer4_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 5 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer5_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 6 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer6_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 7 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer7_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 8 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer8_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 9 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer9_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 10 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer10_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 11 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer11_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 12 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer12_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 13 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer13_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 14 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer14_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 15 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer15_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 16 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer16_attentions.png\n",
      "\n",
      "--- Processing Attention for Layer 17 ---\n",
      "Averaging attention maps for 13 samples of length 9.\n",
      "Using tick step 1 and font size 13 for sequence length 9.\n",
      "\n",
      "Saved grid of attention heatmaps to None/step_100/layer17_attentions.png\n"
     ]
    }
   ],
   "source": [
    "visualize_attention_weights(model, tokenizer, train_dataset, temperature=1.0, step=100, save_path=\"None\", num_samples=100, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58fcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
